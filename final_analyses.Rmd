---
output: pdf_document
header-includes:
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
editor_options: 
  chunk_output_type: console
---

<!-- LaTeX Formatting -->
\fancypagestyle{plain}{\pagestyle{fancy}}
\fancyhead[LO,LE]{Meeraj Kothari \& Will Simmons}
\fancyhead[RO,RE]{P8106: Data Science II Final\newline 14 May 2020}

```{r, echo = FALSE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(
  echo = FALSE, 
  message = FALSE, 
  warning = FALSE
)

```


```{r}

library(tidyverse)
library(conflicted)
conflict_prefer("filter", "dplyr")
library(readr)
library(lubridate)
library(caret)
library(glmnet)
library(e1071)
library(mlbench)
library(pROC)
library(factoextra)
```


## 1. **Introduction**

*Note on updated results.*

For this final project, our group used the same dataset (see below for more info) but binarized our outcome, oceanic oxygen saturation, to **above/below median** for the timerange observed in our analyses. We have modified the text below from our midterm analyses to apply to our new methods and outcome measure.

*Background.*

The consequences of a changing climate are many and far-reaching. One consequence that has gained significant attention over the past decade is ocean deoxygenation, a decrease in levels of dissolved O~2~. The impacts of ocean deoxygenation are only now becoming fully apparent: e.g. changes to ocean biogeochemistry, macro- and microorganism death due to hypoxia, and increased ocean production of nitrous oxide (N~2~O), a greenhouse gas (Keeling et al., 2010).

Thus, we wanted to answer the questions: what factors predict ocean deoxygenation over time? Given these factors, are we able to accurately predict ocean oxygenation in new data? Oceanic levels of O~2~ determine global populations' access to food and other resources, and the ability to predict this information may guide policies by (1) generating potentially-useful information on factors important to preventing or slowing deoxygenation, and (2) demonstrating potential future trends.

We are using data from the California Cooperative Oceanic Fisheries Investigations (CalCOFI), which we downloaded from [\underline{Kaggle}](https://www.kaggle.com/sohier/calcofi). CalCOFI’s data represent the longest oceanographic time series in the world, spanning the years 1949 to 2016. Measured off the coast of California, the dataset includes larval and egg abundance data on 250+ species, as well as oceanographic data such as water temperature, salinity, chlorophyll, and oxygenation.

Our primary outcome for these classification models is dissolved seawater oxygen in µmol/Kg, and whether a given observation is **above/below the median value for our dataset**.. A list of 14 predictors can be found in the Models section.

*Data cleaning and preparation.*

After downloading and importing the data, we cleaned and prepared it. We restricted the dataset to observations after 2008. The original timeseries was quite long (1949-2016, with 800k+ observations) and would have significantly increased computational effort; this also provides a more current (i.e. approximately prior-decade) interpretation of results. Then, since there was class imbalance in our outcome (above/below median oxygen saturation), we used `caret::downsample()` to randomly sample our data so that all classes have the same frequency as the minority class. Finally, for increased computational efficiency, we took a 5% random sample from the resulting downsampled dataset of ~25k, leaving us with ~1,200 observations.

Next, we removed from consideration features with large amounts of missing data. We had selected a preliminary list of predictors, but several of these had large proportions of data missing. Finally, for the 14 features that remained (see Models for the final list), we removed any missing values for predictors. 

After a final dataset was created for analytic purposes, training and testing datasets were created using `caret::createDataPartition()`, with 80 percent of data dedicated to training and 20 percent to testing. The training set was used for all subsequent analyses, including comparison of candidate models, with the testing set only used to evaluate the final chosen model.

Using the training set, we explored outliers for all variables contained in the dataset. While some variables had extremely skewed distributions (e.g. right-skewed phaeophytin, ammonium, and nitrite concentrations), none had outliers visible from boxplots that warranted consideration of correction. Exploratory outlier visualizations are not shown in this document, but code for the boxplots is included in an appendix at the bottom of the submitted RMarkdown file.

<!-- Importing Data -->

```{r}

# Bottle data - multiple bottles per cast
bottle = read_csv('./data/bottle.csv',
                  col_types = cols(.default = col_double()))

# Cast data
cast = read_csv('./data/cast.csv')

```

<!-- Cleaning/preparing data -->

```{r}

# Preparing dataset
bottle_new = 
  bottle %>% 
  select(
    Cst_Cnt,
    Btl_Cnt,
    Sta_ID,
    RecInd,
    R_Depth,
    R_TEMP, T_degC,
    R_POTEMP,
    R_SALINITY, Salnty,
    R_SIGMA, STheta,
    R_CHLA, ChlorA,
    R_PHAEO, Phaeop,
    R_PO4, PO4uM,
    R_SIO3, SiO3uM,
    R_NO2, NO2uM,
    R_NO3, NO3uM,
    R_PRES,
    R_SVA,
    R_NH4, NH3uM,
    C14As1, DarkAs, MeanAs, 
    # DIC1, DIC2, TA1, TA2,      # Almost 100% missing
    R_O2, O2ml_L, 
    R_O2Sat, O2Sat,
    oxy = `Oxy_µmol/Kg`
  ) %>% 
  left_join(
    .,
    cast %>% select(Cst_Cnt, Date, Lat_Dec, Lon_Dec),   # From cast data, only using: join variable, date, lat, long
    by = "Cst_Cnt"
  ) %>% 
  mutate(
    date = as.character(Date),
    date = mdy(date),
    month = floor_date(date, "month"),
    year = year(date),
    month_num = as.factor(month(date))
  ) %>% 
  filter(year >= 2008) %>%  # Reducing size of dataset
  mutate(
    year = as.factor(year)  # Changing year to factor
  )
  
```

<!-- Selecting variables for final dataset -->

```{r}
set.seed(1)

median_oxy = 
  bottle_new %>% 
  select(oxy) %>% 
  summarize(
    median = median(oxy, na.rm = TRUE)
  ) %>% 
  as.numeric()

data_pre =  
  bottle_new %>% 
  mutate(
    outcome = case_when(oxy >= median_oxy ~ 'Above',   # Greater than or equal to median
                        oxy <  median_oxy ~ 'Below'),  # Less than median
    outcome = as.factor(outcome)
  ) %>% 
  select(
    outcome,
    date, 
    month,
    year,
    month_num,
    lat = Lat_Dec, long = Lon_Dec,
    temp_c = R_TEMP,
    sal = R_SALINITY,
    p_dens = R_SIGMA,
    pres = R_PRES,
    phaeo = R_PHAEO,
    po4 = R_PO4,
    sio3 = R_SIO3,
    no2 = R_NO2,
    nh4 = R_NH4,          
    chlor = R_CHLA,
  ) %>% 
  mutate(season = case_when(month_num %in% c(12, 1, 2) ~ 'Winter',
                            month_num %in% c(3:5) ~ 'Spring',
                            month_num %in% c(6:8) ~ 'Summer',
                            month_num %in% c(9:11) ~ 'Fall'),
         season = as.factor(season)
  ) %>% 
  drop_na()           # Removing missing data


data_pre %>% 
  count(outcome)

# Downsampling to create class balance - class membership of data above are 3:1 Above:Below 
set.seed(1)
data_pre_downsampled =
  data_pre %>% 
  select(-date, -month, -month_num) %>% 
  downSample(
    x = .,                                               # Predictors
    y = data_pre$outcome,                                # Factor w/ class memberships
    list = FALSE
  ) %>% 
  as_tibble() %>% 
  select(-Class)                                         # Function creates redundant Class variable

data_pre_downsampled %>% 
  count(outcome)

# Now equal

# Finally, reducing dataframe size to make analyses more manageable
set.seed(1)
data_final =
  data_pre_downsampled %>% 
  sample_frac(0.05)

data_final %>% 
  count(outcome)
# Still approx. 1:1 class ratio

```

<!-- Creating train/test sets -->

```{r}
set.seed(1)
# Creating training and testing datasets

train_idx =
  createDataPartition(data_final$outcome,
                      p = .8,
                      list = F)

# 14 predictors, 1 outcome

train =
  data_final[train_idx[,1], ]

test =
  data_final[-train_idx[,1], ]

rm(train_idx, bottle, bottle_new, cast)

```

## 2. **Exploratory analysis/visualization**

__*2.a.  Correlation.*__

A correlation matrix was created (see Figure 1) to estimate linear (Pearson's) correlation values among all features. Several features were highly correlated, particularly the feature pairs temperature/potential density, SiO~3~/PO~4~, and PO~4~/potential density. Since correlation amongst predictors can cause problems, we were careful to consider these highly-correlated predictors in both model selection and interpretation.

```{r}

train %>% 
  mutate(outcome = as.numeric(outcome),
         year = as.numeric(year),
         season = as.numeric(season)
  ) %>% 
  cor() %>% 
  ggcorrplot::ggcorrplot(type = "lower",
                         colors = c("#ED6A5A", "#FFFFF9", "#36C9C6"),
                         show.diag = FALSE,
                         lab = TRUE,
                         lab_size = 1.75) +
  labs(title = "Figure 1. Correlation matrix of outcome and features",
       subtitle = "CALCOFI, 2008-2016") +
  theme(plot.title.position = "plot",
        legend.position = "bottom") 

```

__*2.b.  Cluster analysis.*__

To explore potential structures and patterns in our data, we performed K-means clustering on our data, determining a `K` value of 3 via the silhouette method.

We performed this cluster analysis on all numeric predictors [latitude and longitude (decimals), temperature (°C), salinity (PSS-78 scale), potential water density (kg/m^3^), pressure (decibars), phaeophytin (µg/L), chlorophyll (µg/L), phosphate (µmol/L), silicate (µmol/L), nitrite (µmol/L), and ammonium (µmol/L)]. 

In Figure 2, we plotted K-means clusters by scaled and centered mean values for numeric predictors. A few interesting patterns emerged, which were complementary to our correlation matrix in Figure 1: certain substances found in water were positively correlated and thus found in certain clusters together, such as `chlor` (chlorophyll), `nh4` (ammonium), and `no2` (nitrite). In addition, certain factors such as `temp_c` (temperature) and `pres` (pressure) were inversely correlated, as would be expected given different sample depths.

```{r}

fviz_nbclust(clust_data,
             FUNcluster = kmeans,
             method = "silhouette")
set.seed(1)
km <- kmeans(clust_data, centers = 3, nstart = 20)

library(viridis)
# km_vis <- fviz_cluster(list(data = clust_data, cluster = km$cluster), 
#                        ellipse.type = "convex", 
#                        geom = c("point","text"),
#                        labelsize = 5, 
#                        palette = "Dark2") + 
#           labs(title = "Figure 2. K-means cluster analysis: Plot of first two principal components",
#                subtitle = "CALCOFI, 2008-2016") +
#           scale_shape("Cluster") + 
#           scale_fill_viridis_d("Cluster") + 
#           scale_color_viridis_d("Cluster") +
#           theme_bw()
# 
# km_vis

a = print(km)
a$centers %>% as_tibble() %>% mutate(Cluster = as.factor(1:3)) %>% select(Cluster, everything()) %>% 
  pivot_longer(
    lat:chlor,
    names_to = "var",
    values_to = "mean"
  ) %>% 
  ggplot(aes(x = var, y = mean, fill = Cluster)) +
  geom_bar(stat = 'identity') +
  facet_grid(~Cluster) +
  labs(x = "",
       title = "Figure 2. K-means cluster analysis: Clusters by centered and scaled mean values",
       subtitle = "CALCOFI, 2008-2016") +
  theme_bw() +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45,
                               vjust = 1, 
                               hjust = 1)
  ) +
  scale_fill_viridis_d("") 

```


## 3. **Models** 

__*3.a.  Candidate models.*__

For each candidate model, the 14 predictors included were as follows: year, season, latitude and longitude (decimals), temperature (°C), salinity (PSS-78 scale), potential water density (kg/m^3^), pressure (decibars), phaeophytin (µg/L), chlorophyll (µg/L), phosphate (µmol/L), silicate (µmol/L), nitrite (µmol/L), and ammonium (µmol/L).

We began by fitting several models using `caret::train()` using our training data, and comparing them using their respective cross-validated (CV) Accuracy and ROC values. **Figure 3** details the cross-validated distribution of model metrics for each model.

We began with a logistic regression model as a baseline model. We then compared this with more flexible methods: K-nearest neighbors (KNN), a support vector classifier with a linear kernel (SVC), and a support vector machine with a radial kernel (SVM). 

*3.a.i.   Logistic Regression* 

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     number = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = T)

set.seed(1)
glm.fit <- train(outcome ~ .,
                 data = train,
                 method = "glm",
                 metric = "ROC",
                 trControl = ctrl)

contrasts(data_final$outcome)

# ROC curve
glm_pred = predict(glm.fit, newdata = train)
glm_roc = roc(train$outcome, as.numeric(glm_pred)) # Have to use numeric version

# plot(glm_roc)
# 
# confusionMatrix(data = glm_pred, 
#                 reference = train$outcome)

```

*3.a.ii.   K-Nearest Neighbors* 

```{r}
set.seed(1)

# knn.fit <- train(outcome ~ .,
#                  data = train, 
#                  method = "knn",
#                  metric = "ROC",
#                  tuneGrid = data.frame(k = seq(1, 100, by = 5)),
#                  trControl = ctrl)

# Saving to make knitting faster
# saveRDS(knn.fit, './models/knn.RDS')
knn.fit = readRDS('./models/knn.RDS')

ggplot(knn.fit, highlight = T)

```

*3.a.iii.  Linear Support Vector Classifier (SVC)*

```{r}

set.seed(1)
# svml = train(outcome ~ .,
#              data = train,
#              method = "svmLinear2",
#              preProcess = c("center", "scale"),
#              metric = "ROC",
#              tuneGrid = data.frame(cost = exp(seq(1, 5, length = 20))),
#              trControl = ctrl)
# beepr::beep(sound = "mario") 

# saveRDS(svml, './models/svml.RDS')
svml = readRDS('./models/svml.RDS')

ggplot(svml, highlight = TRUE)

# Try this to visualize
# kernlab::plot(svml$finalModel,
#      train,
#      formula = outcome ~ chlor)
# plot(svml, train)

```

*3.a.iv.  Radial Kernel Support Vector Machine (SVM)*

```{r}

r_grid = expand.grid(C = exp(seq(1, 8, length = 15)),
                     sigma = exp(seq(-9, -6, length = 10)))

set.seed(1)             
# svmr = train(outcome ~ .,
#              data = train,
#              method = "svmRadial",
#              preProcess = c("center", "scale"),
#              tuneGrid = r_grid,
#              trControl = ctrl)

# saveRDS(svmr, './models/svmr.RDS')
svmr = readRDS('./models/svmr.RDS')

ggplot(svmr, highlight = TRUE)

# Try these to visualize
# plot(svmr$finalModel)
# plot(svmr, train)

```


__*3.b.  Tuning.*__

We tuned each candidate model's tuning parameter(s) using 5 repeats of 10-fold cross validation. The KNN model was tuned over a grid of `K` values ranging from 1 to 100 in increments of 5. The SVC model was tuned over a grid of 20 `cost` values ranging from *exp*(5) to *exp*(8). Finally, the SVM model was tuned over a grid of two parameters: 10 `cost` parameters ranging from *exp*(-1) to *exp*(4), and 10 `sigma` values ranging from *exp*(-6) to *exp*(-2).

Final candidate models were then compared using CV accuracy and ROC distributions, and the final model was selected using the model with the lowest median CV metric values. More details can be found in the submitted RMarkdown file.

```{r}

res <- resamples(list(GLM = glm.fit, 
                      KNN = knn.fit,
                      SVC = svml,
                      SVM = svmr
  )
)

bwplot(res)

```

The following plot shows the test performance of the models using ROC curves: 

```{r}
glm.pred <- predict(glm.fit, newdata = test, type = "prob")[,2]
knn.pred <- predict(knn.fit, newdata = test, type = "prob")[,2]
svml.pred <- predict(svml, newdata = test, type = "prob")[,2]
svmr.pred <- predict(svmr, newdata = test, type = "prob")[,2]

roc.glm <- roc(test$outcome, glm.pred)
roc.knn <- roc(test$outcome, knn.pred)
roc.svml <- roc(test$outcome, svml.pred)
roc.svmr <- roc(test$outcome, svmr.pred)

auc <- c(roc.glm$auc[1], roc.knn$auc[1], roc.svml$auc[1], roc.svmr$auc[1])

plot(roc.glm, legacy.axes = T)
plot(roc.knn, col = 2, add = T)
plot(roc.svml, col = 3, add = T)
plot(roc.svmr, col = 4, add = T)
modelNames <- c("glm", "knn", "svml", "svmr")
legend("bottomright", legend = paste0(modelNames, ":", round(auc, 3)), col = 1:4, lwd = 2)
```

[...]

__*3.c.  Final Model.*__

[...]

*3.c.i.   Test error rate.* Upon evaluating the MARS model using our test data, the test error rate was equal to `r ...`.

*3.c.ii.  Important variables.* The most important variable in our final model, the ___ model, was ___.

*3.c.iii. Interpretation and limitations.* 

(e.g. assumptions of final model, flexibility to capture underlying truth, limitations)

## 4. **Conclusions**

__*4.a.  Summary of findings.*__ 

...

__*4.b.  Limitations of findings.*__

These data were exclusively gathered in the Pacific Ocean off the coast of California; thus, these prediction models and any interpretations may not be generalizable to data outside of this geographic area. Additionally, we limited analyses to data from 2008-2016, which may limit temporal interpretation.

<br>
<br>
<br>

# References

Keeling RF, Kortzinger A,  Gruber N (2010). Ocean Deoxygenation in a Warming World. Annu Rev Marine Sci, 2:463–93.

Watson AJ, Lenton TM, Mills BJW (2017). Ocean deoxygenation, the global phosphorus cycle and the possibility of human-caused large-scale ocean anoxia. Philos Trans A Math Phys Eng Sci, 375(2102).

\newpage

# Appendix: Figures & Tables

<!-- Figure 1: Correlation matrix --> 

```{r, fig.align = 'center', fig.fullwidth = TRUE}
# Correlation plot 
# fig1 =
  train %>% 
  select(year, month_num, everything(), -outcome) %>% 
  rename(NH4 = nh4, NO2 = no2, SiO3 = sio3, PO4 = po4,
         chlorophyll = chlor,
         phaeophytin = phaeo,
         pressure = pres,
         density = p_dens,
         salinity = sal,
         month = month_num) %>% 
  mutate(month = as.numeric(month),
         year = as.numeric(year)) %>% 
  cor() %>% 
  ggcorrplot::ggcorrplot(type = "lower",
                         colors = c("#ED6A5A", "#FFFFF9", "#36C9C6"),
                         show.diag = FALSE,
                         lab = TRUE,
                         lab_size = 1.75) +
  labs(title = "Figure 1. Correlation matrix of features") +
  theme(plot.title.position = "plot",
        legend.position = "bottom")
```




<!-- SEE BELOW FOR UNINCLUDED WORK -->







<!-- ### Appendix 2: Unincluded Work ### -->

<!-- 1. Exploratory Outlier Visualization -->

```{r, include = FALSE, eval = FALSE}

# Outlier analysis using boxplots
# outlier_check = function(predictor, xlab) {
#   
#   train %>% 
#     ggplot(aes_string(y = predictor)) +
#     geom_boxplot() +
#     labs(y = paste0(xlab))
#   
# }
# 
# boxplot_list = pmap(predictor_list, outlier_check)
# Manually call boxplots to assess each variable for outliers, e.g. boxplot_list[[2]]

```

<!-- 2. Gap Statistic Plot Code -->

```{r, include = FALSE, eval = FALSE}

# set.seed(1)
# gap = cluster::clusGap(clust_data, FUN = hcut, nstart = 25, K.max = 15, B = 10)
# gap2 = cluster::clusGap(clust_data, FUN = hcut, nstart = 25, K.max = 15, B = 10, method = "Tibs2001SEmax")
# fviz_gap_stat(gap)
# fviz_gap_stat(gap2)

## Both methods yield k = 3

```

