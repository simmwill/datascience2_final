---
output: pdf_document
header-includes:
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
editor_options: 
  chunk_output_type: console
---

<!-- LaTeX Formatting -->
\fancypagestyle{plain}{\pagestyle{fancy}}
\fancyhead[LO,LE]{Meeraj Kothari \& Will Simmons}
\fancyhead[RO,RE]{P8106: Data Science II Final\newline 14 May 2020}

```{r, echo = FALSE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(
  echo = FALSE, 
  message = FALSE, 
  warning = FALSE
)

```


```{r}

library(tidyverse)
library(conflicted)
conflict_prefer("filter", "dplyr")
library(readr)
library(lubridate)
library(caret)
library(glmnet)
library(e1071)
library(mlbench)
library(pROC)
library(factoextra)

```


## 1. **Introduction**

*Note on updated results.*

For this final project, our group used the same dataset (see below for more info) but binarized our outcome, oceanic oxygen saturation, to **above/below median** for the timerange observed in our analyses. We have modified the text below from our midterm analyses to apply to our new methods and outcome measure.

*Background.*

The consequences of a changing climate are many and far-reaching. One consequence that has gained significant attention over the past decade is ocean deoxygenation, a decrease in levels of dissolved O~2~. The impacts of ocean deoxygenation are only now becoming fully apparent: e.g. changes to ocean biogeochemistry, macro- and microorganism death due to hypoxia, and increased ocean production of nitrous oxide (N~2~O), a greenhouse gas (Keeling et al., 2010).

Thus, we wanted to answer the questions: what factors predict ocean deoxygenation over time? Given these factors, are we able to accurately predict ocean oxygenation in new data? Oceanic levels of O~2~ determine global populations' access to food and other resources, and the ability to predict this information may guide policies by (1) generating potentially-useful information on factors important to preventing or slowing deoxygenation, and (2) demonstrating potential future trends.

We are using data from the California Cooperative Oceanic Fisheries Investigations (CalCOFI), which we downloaded from [\underline{Kaggle}](https://www.kaggle.com/sohier/calcofi). CalCOFI’s data represent the longest oceanographic time series in the world, spanning the years 1949 to 2016. Measured off the coast of California, the dataset includes larval and egg abundance data on 250+ species, as well as oceanographic data such as water temperature, salinity, chlorophyll, and oxygenation.

Our primary outcome for these classification models is dissolved seawater oxygen in µmol/Kg, and whether a given observation is **above/below the median value for our dataset**. A list of 14 predictors can be found in the Models section.

*Data cleaning and preparation.*

After downloading and importing the data, we cleaned and prepared it. We restricted the dataset to observations after 2008. The original timeseries was quite long (1949-2016, with 800k+ observations) and would have significantly increased computational effort; this also provides a more current (i.e. approximately prior-decade) interpretation of results. Then, since there was class imbalance in our outcome (above/below median oxygen saturation), we used `caret::downsample()` to randomly sample our data so that all classes have the same frequency as the minority class. Finally, for increased computational efficiency, we took a 5% random sample from the resulting downsampled dataset of ~25k, leaving us with ~1,200 observations.

Next, we removed from consideration features with large amounts of missing data. We had selected a preliminary list of predictors, but several of these had large proportions of data missing. Finally, for the 14 features that remained (see Models for the final list), we removed any missing values for predictors. 

After a final dataset was created for analytic purposes, training and testing datasets were created using `caret::createDataPartition()`, with 80 percent of data dedicated to training and 20 percent to testing. The training set was used for all subsequent analyses, including comparison of candidate models, with the testing set only used to evaluate the final chosen model.

Using the training set, we explored outliers for all variables contained in the dataset. While some variables had extremely skewed distributions (e.g. right-skewed phaeophytin, ammonium, and nitrite concentrations), none had outliers visible from boxplots that warranted consideration of correction. Exploratory outlier visualizations are not shown in this document, but code for the boxplots is included in an appendix at the bottom of the submitted RMarkdown file.

<!-- Importing Data -->

```{r}

# Bottle data - multiple bottles per cast
bottle = read_csv('./data/bottle.csv',
                  col_types = cols(.default = col_double()))

# Cast data
cast = read_csv('./data/cast.csv')

```

<!-- Cleaning/preparing data -->

```{r}

# Preparing dataset
bottle_new = 
  bottle %>% 
  select(
    Cst_Cnt,
    Btl_Cnt,
    Sta_ID,
    RecInd,
    R_Depth,
    R_TEMP, T_degC,
    R_POTEMP,
    R_SALINITY, Salnty,
    R_SIGMA, STheta,
    R_CHLA, ChlorA,
    R_PHAEO, Phaeop,
    R_PO4, PO4uM,
    R_SIO3, SiO3uM,
    R_NO2, NO2uM,
    R_NO3, NO3uM,
    R_PRES,
    R_SVA,
    R_NH4, NH3uM,
    C14As1, DarkAs, MeanAs, 
    # DIC1, DIC2, TA1, TA2,      # Almost 100% missing
    R_O2, O2ml_L, 
    R_O2Sat, O2Sat,
    oxy = `Oxy_µmol/Kg`
  ) %>% 
  left_join(
    .,
    cast %>% select(Cst_Cnt, Date, Lat_Dec, Lon_Dec),   # From cast data, only using: join variable, date, lat, long
    by = "Cst_Cnt"
  ) %>% 
  mutate(
    date = as.character(Date),
    date = mdy(date),
    month = floor_date(date, "month"),
    year = year(date),
    month_num = as.factor(month(date))
  ) %>% 
  filter(year >= 2008) %>%  # Reducing size of dataset
  mutate(
    year = as.factor(year)  # Changing year to factor
  )
  
```

<!-- Selecting variables for final dataset -->

```{r}
set.seed(1)

median_oxy = 
  bottle_new %>% 
  select(oxy) %>% 
  summarize(
    median = median(oxy, na.rm = TRUE)
  ) %>% 
  as.numeric()

# Alternate oxy median - baseline year (2008)
  # No difference 
# median_oxy_baseline = 
#   bottle_new %>% 
#   select(year, oxy) %>% 
#   mutate(year = as.numeric(as.character(year))) %>% 
#   filter(year == 2008) %>% 
#   summarize(median_baseline = median(oxy, na.rm = TRUE)) %>% 
#   as.numeric()

data_pre =  
  bottle_new %>% 
  mutate(
    outcome = case_when(oxy >= median_oxy ~ 'Above',   # Greater than or equal to median
                        oxy <  median_oxy ~ 'Below'),  # Less than median
    outcome = as.factor(outcome)
  ) %>% 
  select(
    outcome,
    date, 
    month,
    year,
    month_num,
    lat = Lat_Dec, long = Lon_Dec,
    temp_c = R_TEMP,
    sal = R_SALINITY,
    p_dens = R_SIGMA,
    pres = R_PRES,
    phaeo = R_PHAEO,
    po4 = R_PO4,
    sio3 = R_SIO3,
    no2 = R_NO2,
    nh4 = R_NH4,          
    chlor = R_CHLA,
  ) %>% 
  mutate(season = case_when(month_num %in% c(12, 1, 2) ~ 'Winter',
                            month_num %in% c(3:5) ~ 'Spring',
                            month_num %in% c(6:8) ~ 'Summer',
                            month_num %in% c(9:11) ~ 'Fall'),
         season = as.factor(season)
  ) %>% 
  drop_na()           # Removing missing data


# data_pre %>% 
#   count(outcome)

# Downsampling to create class balance - class membership of data above are 3:1 Above:Below 
set.seed(1)
data_pre_downsampled =
  data_pre %>% 
  select(-date, -month, -month_num) %>% 
  downSample(
    x = .,                                               # Predictors
    y = data_pre$outcome,                                # Factor w/ class memberships
    list = FALSE
  ) %>% 
  as_tibble() %>% 
  select(-Class)                                         # Function creates redundant Class variable

# data_pre_downsampled %>% 
#   count(outcome)

# Now equal

# Finally, reducing dataframe size to make analyses more manageable
set.seed(1)
data_final =
  data_pre_downsampled %>% 
  sample_frac(0.05)

# data_final %>% 
#   count(outcome)
# Still approx. 1:1 class ratio

```

<!-- Creating train/test sets -->

```{r}
set.seed(1)
# Creating training and testing datasets

train_idx =
  createDataPartition(data_final$outcome,
                      p = .8,
                      list = F)

# 14 predictors, 1 outcome

train =
  data_final[train_idx[,1], ]

test =
  data_final[-train_idx[,1], ]

rm(train_idx, bottle, bottle_new, cast)

```

## 2. **Exploratory analysis/visualization**

__*2.a.  Correlation.*__

We created a correlation matrix (**Figure 1**) to estimate linear (Pearson's) correlation values among all features and a binary measure of our outcome. Several features were highly correlated, particularly the feature pairs temperature/potential density, SiO~3~/PO~4~, and PO~4~/potential density. Since correlation amongst predictors can cause problems, we were careful to consider these highly-correlated predictors in both model selection and interpretation.

```{r}
# Figure 1 below in Appendix 1

# train %>% 
#   mutate(outcome = as.numeric(outcome),
#          year = as.numeric(year),
#          season = as.numeric(season)
#   ) %>% 
#   cor() %>% 
#   ggcorrplot::ggcorrplot(type = "lower",
#                          colors = c("#ED6A5A", "#FFFFF9", "#36C9C6"),
#                          show.diag = FALSE,
#                          lab = TRUE,
#                          lab_size = 1.75) +
#   labs(title = "Figure 1. Correlation matrix of outcome and features",
#        subtitle = "CALCOFI, 2008-2016") +
#   theme(plot.title.position = "plot",
#         legend.position = "bottom") 

```

__*2.b.  Cluster analysis.*__

To explore potential structures and patterns in our data, we performed K-means clustering on our data, determining a `K` value of 3 via the silhouette method. (See **Figure S1** for our K-means tuning parameter plot.)

We performed this cluster analysis on all numeric predictors [latitude and longitude (decimals), temperature (°C), salinity (PSS-78 scale), potential water density (kg/m^3^), pressure (decibars), phaeophytin (µg/L), chlorophyll (µg/L), phosphate (µmol/L), silicate (µmol/L), nitrite (µmol/L), and ammonium (µmol/L)]. 

In **Figure 2**, we plotted K-means clusters by scaled and centered mean values for numeric predictors. A few interesting patterns emerged, which were complementary to our correlation matrix in **Figure 1**: certain substances found in water were positively correlated and thus found in certain clusters together, such as `chlor` (chlorophyll), `nh4` (ammonium), and `no2` (nitrite). In addition, certain factors such as `temp_c` (temperature) and `pres` (pressure) were inversely correlated, as would be expected given different sample depths.

```{r}

clust_data =
  train %>% 
  select_if(is.numeric) %>% 
  scale()

# Figure S1 in Appendix II below
# fviz_nbclust(clust_data,
#              FUNcluster = kmeans,
#              method = "silhouette") +
#   labs(title = "Figure S1. Tuning K-means cluster analysis via silhouette method",
#        subtitle = "Optimal k value of 3")

set.seed(1)
km <- kmeans(clust_data, centers = 3, nstart = 20)

library(viridis)
# km_vis <- fviz_cluster(list(data = clust_data, cluster = km$cluster), 
#                        ellipse.type = "convex", 
#                        geom = c("point","text"),
#                        labelsize = 5, 
#                        palette = "Dark2") + 
#           labs(title = "Figure 2. K-means cluster analysis: Plot of first two principal components",
#                subtitle = "CALCOFI, 2008-2016") +
#           scale_shape("Cluster") + 
#           scale_fill_viridis_d("Cluster") + 
#           scale_color_viridis_d("Cluster") +
#           theme_bw()
# 
# km_vis

# Figure 2 below in Appendix

```

<!-- __*2.c.  Predictor scatterplots*__ -->

```{r}


# Figure 3 below in Appendix


```

## 3. **Models** 

__*3.a.  Candidate models.*__

For each candidate model, the 14 predictors included were as follows: year, season, latitude and longitude (decimals), temperature (°C), salinity (PSS-78 scale), potential water density (kg/m^3^), pressure (decibars), phaeophytin (µg/L), chlorophyll (µg/L), phosphate (µmol/L), silicate (µmol/L), nitrite (µmol/L), and ammonium (µmol/L).

We began by fitting several models using `caret::train()` using our training data, and comparing them using their respective cross-validated (CV) ROC values. **Figure 3** details the cross-validated distribution of model metrics for each model.

We began with a logistic regression model as a baseline model. We then compared this with more flexible methods: K-nearest neighbors (KNN), a support vector classifier with a linear kernel (SVC), and a support vector machine with a radial kernel (SVM). 

<!-- *3.a.i.   Logistic Regression*  -->

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     number = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = T)

set.seed(1)
glm.fit <- train(outcome ~ .,
                 data = train,
                 method = "glm",
                 metric = "ROC",
                 trControl = ctrl)

# contrasts(data_final$outcome)

# ROC curve
glm_pred = predict(glm.fit, newdata = train)
glm_roc = roc(train$outcome, as.numeric(glm_pred)) # Have to use numeric version

# plot(glm_roc)
# 
# confusionMatrix(data = glm_pred, 
#                 reference = train$outcome)

```

<!-- *3.a.ii.   K-Nearest Neighbors*  -->

```{r}
set.seed(1)

# knn.fit <- train(outcome ~ .,
#                  data = train, 
#                  method = "knn",
#                  metric = "ROC",
#                  tuneGrid = data.frame(k = seq(1, 100, by = 5)),
#                  trControl = ctrl)

# Saving to make knitting faster
# saveRDS(knn.fit, './models/knn.RDS')
knn.fit = readRDS('./models/knn.RDS')

# Figure S2 below in Appendix II
# ggplot(knn.fit, highlight = T) +
#   theme_bw() +
#   labs(title = "Figure S2. Tuning K-nearest neighbor model's `k` parameter using ROC",
#        subtitle = "Optimal k value of 11")

```

<!-- *3.a.iii.  Linear Support Vector Classifier (SVC)* -->

```{r}

set.seed(1)
# svml = train(outcome ~ .,
#              data = train,
#              method = "svmLinear2",
#              preProcess = c("center", "scale"),
#              metric = "ROC",
#              tuneGrid = data.frame(cost = exp(seq(1, 5, length = 20))),
#              trControl = ctrl)
# beepr::beep(sound = "mario") 

# saveRDS(svml, './models/svml.RDS')
svml = readRDS('./models/svml.RDS')

# Figure S3 below in Appendix II
# ggplot(svml, highlight = TRUE) +
#   theme_bw() +
#   labs(title = "Figure S3. Tuning linear support vector classifier model's cost parameter using ROC",
#        subtitle = "Optimal cost value of 6.31")

```

<!-- *3.a.iv.  Radial Kernel Support Vector Machine (SVM)* -->

```{r}

r_grid = expand.grid(C = exp(seq(1, 8, length = 15)),
                     sigma = exp(seq(-9, -6, length = 10)))

set.seed(1)             
# svmr = train(outcome ~ .,
#              data = train,
#              method = "svmRadial",
#              preProcess = c("center", "scale"),
#              tuneGrid = r_grid,
#              trControl = ctrl)

# saveRDS(svmr, './models/svmr.RDS')
svmr = readRDS('./models/svmr.RDS')

# Figure S4 in Appendix II below
# ggplot(svmr, highlight = TRUE) +
#   theme_bw() +
#   labs(title = "Figure S4. Tuning radial support vector machine's cost and sigma parameters using ROC",
#        subtitle = "Cost = 244.69, sigma = 0.00248")

```

__*3.b.  Tuning.*__

We tuned each candidate model's tuning parameter(s) using 5 repeats of 5-fold cross validation. The KNN model was tuned over a grid of `K` values ranging from 1 to 100 in increments of 5. The SVC model was tuned over a grid of 20 `cost` values ranging from *exp*(5) to *exp*(8). Finally, the SVM model was tuned over a grid of two parameters: 10 `cost` parameters ranging from *exp*(-1) to *exp*(4), and 10 `sigma` values ranging from *exp*(-6) to *exp*(-2). See Appendix II, **Supplemental Figures S2-S4** for tuning parameter plots for tuned models.

Final candidate models were then compared using CV ROC distributions, and the final model was selected using the model with the highest median CV ROC value. **Figure 4** shows ROC, Sensitivity, and Specificity CV distributions for our tuned models.

```{r}

res <- resamples(list(GLM = glm.fit, 
                      KNN = knn.fit,
                      SVC = svml,
                      SVM = svmr
  )
)

# Figure 4 below in Appendix

```

See **Supplemental Figure S5** for information on model performance in test data.

```{r}
glm.pred <- predict(glm.fit, newdata = test, type = "prob")[,2]
knn.pred <- predict(knn.fit, newdata = test, type = "prob")[,2]
svml.pred <- predict(svml, newdata = test, type = "prob")[,2]
svmr.pred <- predict(svmr, newdata = test, type = "prob")[,2]

roc.glm <- roc(test$outcome, glm.pred)
roc.knn <- roc(test$outcome, knn.pred)
roc.svml <- roc(test$outcome, svml.pred)
roc.svmr <- roc(test$outcome, svmr.pred)

auc <- c(roc.glm$auc[1], roc.knn$auc[1], roc.svml$auc[1], roc.svmr$auc[1])

# only move code below

# Figure S5 ROC test plots in Appendix 2 Supplementary Figures below
# plot(roc.glm, legacy.axes = T)
# plot(roc.knn, col = 2, add = T)
# plot(roc.svml, col = 3, add = T)
# plot(roc.svmr, col = 4, add = T)
# modelNames <- c("glm", "knn", "svml", "svmr")
# legend("bottomright", legend = paste0(modelNames, ": ", round(auc, 3)), col = 1:4, lwd = 2)
# title(main = "Figure S5. ROC curves for all tuned models, test data", adj = 0, line = 2.5)

```

__*3.c.  Final Model.*__

Using median ROC values found by repeated CV in our training data, our best model was the logistic regression model (noted GLM in figures). Of our models, the GLM has perhaps the strictest assumptions (see section *3.c.iii.* below.)

*3.c.i.   Test ROC value.* Upon evaluating the GLM using our test data, the test ROC value was equal to `r roc.glm$auc[1]`.

*3.c.ii.  Important variables.* **Figure 5** shows a variable importance plot of our GLM predictors, using the absolute value of the t-statistic for each model parameter as a measure of importance. Although this is an imperfect measure of variable importance, it gives a decent idea of which variables were most predictive of below-median oxygenation.

```{r}

var_imp = varImp(glm.fit)

# Figure 5 below in Appendix

```

*3.c.iii. Visualization, interpretation, and limitations.* 

**Figure 6** shows marginal predicted probability plots for selected predictors in our logistic model. 
The logistic regression model does not require the assumptions of the OLS models. Binary logistic regression requires the dependent variable to be binary and the class probabilities of the dependent variable must be independent of each other. A major limitation of logistic regression is that it is unable to solve non-linear problems as it assumes a linear decision boundary. It is also unable to handle multiple collinearity among the predictors as efficiently. 

In **Figure 6**, we see that as salinity increases, probability of below-median oxygenation decreases; for temperature, potential water density, phosphates, silicates, and chlorophyll, this effect is opposite (i.e. as these increase, probability of below-median oxygenation increases). Ammonium and nitrite had little predictive ability. However, it is important to note that such marginal predicted probability plots are average trends for each predictor holding other variables constant. In addition, there was high correlation among features, which may prevent interpretation of individual predictors.

```{r, results = FALSE}
library(patchwork)
library(margins)

# # Predicted marginal probability plots
# wrap_elements(~cplot(glm.model, "sal", se.type = "none", main = "Salinity")) +
# wrap_elements(~cplot(glm.model, "temp_c", se.type = "none", main = "Temperature")) +
# wrap_elements(~cplot(glm.model, "p_dens", se.type = "none", main = "Potential density")) +
# wrap_elements(~cplot(glm.model, "po4", se.type = "none", main = "PO4")) +
# wrap_elements(~cplot(glm.model, "sio3", se.type = "none", main = "SiO3")) +
# wrap_elements(~cplot(glm.model, "chlor", se.type = "none", main = "Chlorophyll"))

# Same model as in train()
glm.model = glm(data = train,
                formula = outcome ~ .,
                family = binomial)

marginal = function(predictor, title) {
  
  cdata = cplot(glm.model, paste0(predictor), draw = FALSE)
  
  ggplot(data = cdata, 
         aes(x = xvals, y = yvals)
  ) +
    geom_line() +
    labs(title = paste0(title),
         x = "",
         y = "") +
    theme_bw() +
    ylim(0,1) +
    theme(
      plot.title = element_text(hjust = 0.5)
    )
  
}

marginal_grid =
  marginal("sal", "Salinity") + 
  marginal("temp_c", "Temperature") + 
  marginal("p_dens", "Potential density") + 
  marginal("po4", "Phosphate") +
  marginal("sio3", "Silicate") +
  marginal("chlor", "Chlorophyll") +
  marginal("nh4", "Ammonium") +
  marginal("no2", "Nitrite")
  
# Figure 6 below in Appendix

```

## 4. **Conclusions**

__*4.a.  Summary of findings.*__ 

In this analysis, we have evaluated several models to predict whether the ocean oxygenation level will be above or below the median of the year between 2006-2016 given a set of predictors. Based on cross-validation, the generalized linear model had the highest training and test ROC values among the fitted models. This may be related to the fact that the magnitude of pearson's correlation coefficients between outcome class membership and the predictors is considerably high, as shown in **Figure 1** of the Appendix. Among the 14 predictors, the year, longitude, and concentration of PO~4~ and NH~4~ were highly predictive of  oxygenation level. We also found that the prediction accuracy of the SVM with Linear Kernel was higher than the SVM with Radial Kernel. This further supports that the predictors might be linearly associated with the log odds of oxygenation being above or below the median. 

__*4.b.  Limitations of findings.*__

These data were exclusively gathered in the Pacific Ocean off the coast of California; thus, these prediction models and any interpretations may not be generalizable to data outside of this geographic area. Additionally, we limited analyses to data from 2008-2016, which may limit temporal interpretation.

<br>
<br>
<br>

# References

Keeling RF, Kortzinger A,  Gruber N (2010). Ocean Deoxygenation in a Warming World. Annu Rev Marine Sci, 2:463–93.

Watson AJ, Lenton TM, Mills BJW (2017). Ocean deoxygenation, the global phosphorus cycle and the possibility of human-caused large-scale ocean anoxia. Philos Trans A Math Phys Eng Sci, 375(2102).

\newpage

# Appendix: Main Figures

<!-- Figure 1: Correlation matrix --> 

```{r, fig.align = 'center', fig.width = 12, fig.height = 12}
# Correlation plot 

train %>%
  mutate(outcome = as.numeric(outcome),
         year = as.numeric(year),
         season = as.numeric(season)
  ) %>%
  cor() %>%
  ggcorrplot::ggcorrplot(type = "lower",
                         colors = c("#ED6A5A", "#FFFFF9", "#36C9C6"),
                         show.diag = FALSE,
                         lab = TRUE,
                         lab_size = 4) +
  labs(title = "Figure 1. Correlation matrix of outcome and features",
       subtitle = "CALCOFI, 2008-2016") +
  theme(plot.title.position = "plot",
        legend.position = "bottom")

```

\newpage

<!-- Figure 2: K-means cluster graph -->

```{r, results = FALSE}
a = print(km)
```
```{r}
a$centers %>% as_tibble() %>% mutate(Cluster = as.factor(1:3)) %>% select(Cluster, everything()) %>% 
rename(NH4 = nh4, NO2 = no2, SiO3 = sio3, PO4 = po4,
     Temperature = temp_c,
     Chlorophyll = chlor,
     Phaeophytin = phaeo,
     Pressure = pres,
     Density = p_dens,
     Salinity = sal,
     Latitude = lat, Longitude = long) %>% 
  pivot_longer(
    Latitude:Chlorophyll,
    names_to = "var",
    values_to = "mean"
  ) %>% 
  ggplot(aes(x = var, y = mean, fill = Cluster)) +
  geom_bar(stat = 'identity') +
  facet_grid(~Cluster) +
  labs(x = "",
       y = "Centered/scaled mean value",
       title = "Figure 2. K-means cluster analysis: Clusters by centered and scaled mean values",
       subtitle = "CALCOFI, 2008-2016") +
  theme_bw() +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45,
                               vjust = 1, 
                               hjust = 1)
  ) +
  scale_fill_viridis_d("") 

```

\newpage

<!-- Figure 3. Exploratory scatterplot trellis with featurePlot() -->

```{r, fig.width = 12, fig.height = 12}

featurePlot(x = train %>% select_if(is.numeric) %>% select(-lat,-long),
            plot = "pairs",
            y = train$outcome,
            auto.key = list(columns = 2),
            main = list(
              "Figure 3. Scatterplot matrix: predictors vs. outcome (below-median ocean oxygenation)",
              adj = 0),
            scales = list(
              x = list(cex = .1)
              )
            )

```

\newpage

<!-- Figure 4. Resample plot -->

```{r}

bwplot(res,
       main = "Figure 4. Median resample metrics, training set (5-fold repeated CV)")

```

\newpage

<!-- Figure 5. Variable importance, GLM -->

```{r}

ggplot(var_imp) +
  theme_bw() +
  labs(title = "Figure 5. Variable importance: Logistic (GLM) model, training data",
       subtitle = "Year, PO4, NH4, and longitude were particularly important")

```

\newpage

<!-- Figure 6. Marginal probability plots -->

```{r, fig.height = 7}

marginal_grid %>% wrap_elements() +
  labs(title = "Figure 6. Predicted marginal probability of below-median oxygenation",
       subtitle = "By selected predictors")

```

\newpage 

# Appendix II: Supplementary Figures

<!-- Figure S1. Tuning: K-means cluster analysis -->

```{r}

fviz_nbclust(clust_data,
             FUNcluster = kmeans,
             method = "silhouette") +
  labs(title = "Figure S1. Tuning K-means cluster analysis via silhouette method",
       subtitle = "Optimal k value of 3")

```

\newpage

<!-- Figure S2. KNN tuning plot -->

```{r}

ggplot(knn.fit, highlight = T) +
  theme_bw() +
  labs(title = "Figure S2. Tuning K-nearest neighbor model's `k` parameter using ROC",
       subtitle = "Optimal k value of 11")

```

\newpage

<!-- Figure S3. SVML tuning plot -->

```{r}

ggplot(svml, highlight = TRUE) +
  theme_bw() +
  labs(title = "Figure S3. Tuning linear support vector classifier model's cost parameter using ROC",
       subtitle = "Optimal cost value of 6.31")

```

\newpage

<!-- Figure S4. SVMR tuning plot -->

```{r}

ggplot(svmr, highlight = TRUE) +
  theme_bw() +
  labs(title = "Figure S4. Tuning radial support vector machine's cost and sigma parameters using ROC",
       subtitle = "Cost = 244.69, sigma = 0.00248")

```

\newpage

<!-- Figure S5. ROC test curves -->

```{r}

plot(roc.glm, legacy.axes = T)
plot(roc.knn, col = 2, add = T)
plot(roc.svml, col = 3, add = T)
plot(roc.svmr, col = 4, add = T)
modelNames <- c("glm", "knn", "svml", "svmr")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc, 3)), col = 1:4, lwd = 2)
title(main = "Figure S5. ROC curves for all tuned models, test data", adj = 0, line = 2.5)

```





<!-- SEE BELOW FOR UNINCLUDED WORK -->







<!-- ### Appendix 2: Unincluded Work ### -->

<!-- 1. Exploratory Outlier Visualization -->

```{r, include = FALSE, eval = FALSE}

# Outlier analysis using boxplots
# outlier_check = function(predictor, xlab) {
#   
#   train %>% 
#     ggplot(aes_string(y = predictor)) +
#     geom_boxplot() +
#     labs(y = paste0(xlab))
#   
# }
# 
# boxplot_list = pmap(predictor_list, outlier_check)
# Manually call boxplots to assess each variable for outliers, e.g. boxplot_list[[2]]

```

<!-- 2. Gap Statistic Plot Code -->

```{r, include = FALSE, eval = FALSE}

# set.seed(1)
# gap = cluster::clusGap(clust_data, FUN = hcut, nstart = 25, K.max = 15, B = 10)
# gap2 = cluster::clusGap(clust_data, FUN = hcut, nstart = 25, K.max = 15, B = 10, method = "Tibs2001SEmax")
# fviz_gap_stat(gap)
# fviz_gap_stat(gap2)

## Both methods yield k = 3

```

